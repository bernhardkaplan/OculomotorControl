{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import utils\n",
      "import simulation_parameters\n",
      "import matplotlib\n",
      "import BasalGanglia\n",
      "GP = simulation_parameters.global_parameters()\n",
      "params = GP.params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "n_hc: 50\tn_mc_per_hc: 50\tn_mc: 2500\tn_exc_per_mc: 1\n",
        "n_cells_mpn: 2550\tn_exc_mpn: 2500\tn_inh_mpn: 50\n",
        "n_inh_mpn / n_exc_mpn = 0.020\tn_inh_mpn / n_cells_mpn = 0.020\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BG = BasalGanglia.BasalGanglia(params, dummy=True)\n",
      "action_bins = BG.action_bins_x\n",
      "print BG.map_suboptimal_action"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'Training_RBL_titer100_nRF50_nV50_2_nStim2x1_taup200/Parameters/suboptimal_action_mapping.json'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-18-980d111c19b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasalGanglia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasalGanglia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maction_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_bins_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mBG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_suboptimal_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/bernhard/workspace/OculomotorControl/BasalGanglia.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, comm, dummy)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'basal_ganglia_seed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_suboptimal_action_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_action_speed_mapping_bins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/bernhard/workspace/OculomotorControl/BasalGanglia.py\u001b[0m in \u001b[0;36mcreate_suboptimal_action_mapping\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_suboptimal_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'suboptimal_training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_plus_minus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0moutput_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bg_suboptimal_action_mapping_fn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_suboptimal_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'Training_RBL_titer100_nRF50_nV50_2_nStim2x1_taup200/Parameters/suboptimal_action_mapping.json'"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_next_stim(params, stim_params, v_eye):\n",
      "    x_stim = stim_params[0] - (stim_params[2] - v_eye) * params['t_iteration'] / params['t_cross_visual_field']\n",
      "    return (x_stim, stim_params[1], stim_params[2], stim_params[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_reward_from_action(chosen_action, stim_params, action_bins):\n",
      "    all_outcomes = np.zeros(len(action_bins))\n",
      "    for i_, action in enumerate(action_bins):    \n",
      "        all_outcomes[i_] = get_next_stim(params, stim_params, action)[0]\n",
      "    best_action = np.argmin(np.abs(all_outcomes - .5))\n",
      "    if chosen_action != best_action:\n",
      "        return -1.\n",
      "    else:\n",
      "        return 1."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_reward_from_stimulus(x0, x1, learning_rate=1., punish_overshoot=1.):\n",
      "    dx_i = x0 - .5\n",
      "    dx_j = x1 - .5\n",
      "    dx_i_abs = np.abs(dx_i)\n",
      "    dx_j_abs = np.abs(dx_j)\n",
      "    diff_dx_abs = dx_j_abs - dx_i_abs\n",
      "    R = -1. * learning_rate * diff_dx_abs\n",
      "    if np.sign(dx_i) != np.sign(dx_j):\n",
      "        R *= punish_overshoot\n",
      "    return R"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_0 = .6\n",
      "stim_params = (x_0, .5, .3, .0)\n",
      "learning_rate = 10.\n",
      "punish_overshoot = 1.\n",
      "\n",
      "rewards = np.zeros((len(action_bins), 2))\n",
      "\n",
      "stim_pos = np.zeros(len(action_bins))\n",
      "\n",
      "for action_idx, action in enumerate(action_bins):    \n",
      "    next_mp = get_next_stim(params, stim_params, action)\n",
      "    reward_action = get_reward_from_action(action_idx, stim_params, action_bins)\n",
      "    \n",
      "    x0 = stim_params[0]\n",
      "    x1 = next_mp[0]\n",
      "    stim_pos[action_idx] = x1\n",
      "    reward_stim = get_reward_from_stimulus(x0, x1, learning_rate, punish_overshoot)\n",
      "    rewards[action_idx, 0] = reward_action\n",
      "    rewards[action_idx, 1] = reward_stim\n",
      "    #print action, '\\t', next_mp, '\\t', best_action, reward_stim"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'action_bins' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-8473ccf21c34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpunish_overshoot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstim_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'action_bins' is not defined"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_val = np.min(rewards[:, 1])\n",
      "max_val = np.max(rewards[:, 1])\n",
      "\n",
      "#min_val = -1\n",
      "#max_val = 1\n",
      "\n",
      "norm = matplotlib.colors.Normalize(vmin=min_val, vmax=max_val)\n",
      "m = matplotlib.cm.ScalarMappable(norm=norm, cmap=matplotlib.cm.jet)\n",
      "m.set_array(np.arange(min_val, max_val, 0.01))\n",
      "\n",
      "fig = pylab.figure(figsize=(6, 10))\n",
      "ax = fig.add_subplot(111)\n",
      "for i_, action in enumerate(action_bins):\n",
      "    \n",
      "    if rewards[i_, 0] < 0:\n",
      "        c_a = 'b'\n",
      "        lw_a = 1\n",
      "    else:\n",
      "        c_a = 'r'            \n",
      "        lw_a = 5\n",
      "    ax.plot((0, 1), (x_0 - .5, stim_pos[i_] - .5), c=c_a, lw=lw_a)\n",
      "    \n",
      "    if rewards[i_, 1] < 0:\n",
      "        ls_s = ':'\n",
      "    else:\n",
      "        ls_s = '-'\n",
      "    c_s = m.to_rgba(rewards[i_, 1])\n",
      "    print 'Action %d rewards stim_based %.3f' % (i_, rewards[i_, 1])\n",
      "    ax.plot((2, 3), (x_0 -.5, stim_pos[i_] - .5), c=c_s, ls=ls_s, label='%.1f' % (rewards[i_, 1]))\n",
      "\n",
      "\n",
      "ax.plot((0, 3), (.0, .0), ls='--', c='k', lw=1)    \n",
      "ax.set_xlabel('Iteration, [a.u.]')\n",
      "ax.set_ylabel('Retinal displacement')\n",
      "ax.set_title('Reward policy comparison, Red=high Kappa, Blue=negative Kappa')\n",
      "ax.set_xlim((0, 4))\n",
      "pylab.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'rewards' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-11-395832295ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#min_val = -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#max_val = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'rewards' is not defined"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}